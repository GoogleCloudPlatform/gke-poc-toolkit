# ðŸš² GKE Poc Toolkit Demo: GKE Fleet setup with ConfigSync and Argo Rollouts
This demo shows you how to bootstrap a Fleet of GKE clusters using Config Sync as your gitops engine and Argo Rollouts to progressively release app updates.
Services in play:
* [ConfigSync](https://cloud.google.com/anthos-config-management/docs/config-sync-overview)
* [Argo Rollouts](https://argoproj.github.io/argo-rollouts/)
* [GKE](https://cloud.google.com/kubernetes-engine/docs)
* [Multi Cluster Services](https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-services)
* [Multi Cluster Ingress](https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress)
* [Anthos Service Mesh w/ Managed Control Plane](https://cloud.google.com/service-mesh/docs/overview#managed_anthos_service_mesh)



![diagram](assets/diagram.png)

## Prereqs
1. Setup Cloud Identity for groups or a Workforce Identity Federation pool if you want to BYO IDP.
2. Create users and groups for a team-whereami and team-llm-inferencing


## Fleet Infra setup

1. **Initiliaze the GKE POC Toolkit (gkekitctl init).** 
```bash
export GKE_PROJECT_ID=<your-project-id>
export OS="darwin" # choice of darwin or amd64
```

```bash
gcloud config set project $GKE_PROJECT_ID
gcloud auth login
gcloud auth application-default login

ROOT_DIR=`pwd`
mkdir gke-poc-toolkit && cd "$_"
VERSION=$(curl -s https://api.github.com/repos/GoogleCloudPlatform/gke-poc-toolkit/releases/latest | grep browser_download_url | cut -d "/" -f 8 | tail -1)
curl -sLSf -o ./gkekitctl https://github.com/GoogleCloudPlatform/gke-poc-toolkit/releases/download/${VERSION}/gkekitctl-${OS} && chmod +x ./gkekitctl
mv config.yaml ./gke-poc-toolkit/config.yaml
./gkekitctl init
```

2. **Configure the default Config Sync repo.**
```bash
# Set up self signed cert for ASM Ingress Gateway
mkdir $ROOT_DIR/tmp
openssl req -new -newkey rsa:4096 -days 365 -nodes -x509 \
-subj "/CN=frontend.endpoints.${GKE_PROJECT_ID}.cloud.goog/O=Edge2Mesh Inc" \
-keyout $ROOT_DIR/tmp/frontend.endpoints.${GKE_PROJECT_ID}.cloud.goog.key \
-out $ROOT_DIR/tmp/frontend.endpoints.${GKE_PROJECT_ID}.cloud.goog.crt

gcloud secrets create edge2mesh-credential-crt --replication-policy="automatic" --data-file="$ROOT_DIR/tmp/frontend.endpoints.${GKE_PROJECT_ID}.cloud.goog.crt" --project ${GKE_PROJECT_ID}
gcloud secrets create edge2mesh-credential-key --replication-policy="automatic" --data-file="$ROOT_DIR/tmp/frontend.endpoints.${GKE_PROJECT_ID}.cloud.goog.key" --project ${GKE_PROJECT_ID}

rm -rf tmp

# Set Fleet project in the gkekitctl and default root sync configs
cd ${ROOT_DIR}
export CLOUD_IDENTITY_GROUP=<your-cloud-identity-group>
find "${ROOT_DIR}" -type f -name "*.yaml" -print0 | while IFS= read -r -d '' file; do
  if [[ "$OSTYPE" == "darwin"* ]]; then
    sed -i '' -e "s|MYPROJECT|${GKE_PROJECT_ID}|g" "${file}"
    sed -i '' -e "s|MYCLOUDIDENTITYGROUP|${CLOUD_IDENTITY_GROUP}|g" "${file}"
  else
    sed -i -e "s|MYPROJECT|${GKE_PROJECT_ID}|g" "${file}"
    sed -i -e "s|MYCLOUDIDENTITYGROUP|${CLOUD_IDENTITY_GROUP}|g" "${file}"
  fi
done
```

3. **Run the gkekitctl create command from this directory.** This will take about 15 minutes to run.
```bash
cd ${ROOT_DIR}/gke-poc-toolkit
./gkekitctl apply --config config.yaml
```

4. **We highly recommend installing [kubectx and kubens](https://github.com/ahmetb/kubectx) to switch kubectl contexts between clusters with ease. Once done, you can validate you clusters like so.**
```bash
# validate all clusters are in the kubeconfig generated by gktkitctl
export KUBECONFIG=${ROOT_DIR}/gke-poc-toolkit/kubeconfig

# validate the admin controller install
kubectx connectgateway_${GKE_PROJECT_ID}_us-central1_gke-ap-admin-cp-00
kubectl get nodes
```

*Expected output for each cluster*: 
```bash
NAME                                                  STATUS   ROLES    AGE   VERSION
gke-mccp-central-01-linux-gke-toolkit-poo-12b0fa78-grhw   Ready    <none>   11m   v1.21.6-gke.1500
gke-mccp-central-01-linux-gke-toolkit-poo-24d712a2-jm5g   Ready    <none>   11m   v1.21.6-gke.1500
gke-mccp-central-01-linux-gke-toolkit-poo-6fb11d07-h6xb   Ready    <none>   11m   v1.21.6-gke.1500
```

4. **Push config to the default config sync gcp repo created during the gkekitctl create command.**
```bash
gcloud source repos clone default-config-sync-repo --project ${GKE_PROJECT_ID}
mv ${ROOT_DIR}/default-configs ${ROOT_DIR}/default-config-sync-repo
mv ${ROOT_DIR}/teams ${ROOT_DIR}/ 
cd ${ROOT_DIR}/default-config-sync-repo
git add . && git commit -m "initial push" && git push

# if you have the nomos tool install you can use this to check the status of the config sync root repo
nomos status
```

## Multi cluster load balancing demo
stuffs

1. **Create the Whereami Team and binde the Whereami team to a cluster that is not the closest to your location.**
```bash
# grant source repo access to the whereami frontend and backend KSAs
gcloud iam service-accounts add-iam-policy-binding \
    cs-service-account@gke-toolkit-test-nonsharedvpc.iam.gserviceaccount.com \
    --role=roles/iam.workloadIdentityUser \
    --member="serviceAccount:gke-toolkit-test-nonsharedvpc.svc.id.goog[config-management-system/ns-reconciler-whereami-frontend-whereami-frontend-17" \
    --project=gke-toolkit-test-nonsharedvpc
gcloud iam service-accounts add-iam-policy-binding \
    cs-service-account@gke-toolkit-test-nonsharedvpc.iam.gserviceaccount.com \
    --role=roles/iam.workloadIdentityUser \
    --member="serviceAccount:gke-toolkit-test-nonsharedvpc.svc.id.goog[config-management-system/ns-reconciler-whereami-frontend-whereami-backend-16" \
    --project=gke-toolkit-test-nonsharedvpc

gcloud container fleet scopes create team-whereami --project ${GKE_PROJECT_ID}
gcloud container fleet scopes namespaces create whereami-frontend --scope=team-whereami --project ${GKE_PROJECT_ID} 
gcloud container fleet memberships bindings create gke-ap-central-00-team-whereami \
  --membership gke-ap-central-00 \
  --scope  team-whereami \
  --location us-central1 \
  --project ${GKE_PROJECT_ID}
gcloud beta container fleet scopes add-app-operator-binding SCOPE_ID \
  --role=ROLE \
  --group=${TEAM_EMAIL} \
  --project=${GKE_PROJECT_ID}
```

2. **Validate that the whereami app is alive and that your requests are getting routed to the GKE cluster you bound go the team scope.**
```bash
curl https://whereami.endpoints.${GKE_PROJECT_ID}.cloud.goog/
# The output should look something like this...
{
  "cluster_name": "gke-std-west02", 
  "host_header": "whereami.endpoints.argo-spike.cloud.goog", 
  "pod_name": "whereami-rollout-6d6cb979b5-5xzpj", 
  "pod_name_emoji": "ðŸ‡¨ðŸ‡µ", 
  "project_id": "argo-spike", 
  "timestamp": "2022-08-01T16:16:56", 
  "zone": "us-west1-b"
}
```

3. **Bind the GKE cluster that is closest to your locationto the Team and validate that requests are getting routed to it.**
```bash
curl https://whereami.endpoints.${GKE_PROJECT_ID}.cloud.goog/
# The output should look something like this...
{

}
```

