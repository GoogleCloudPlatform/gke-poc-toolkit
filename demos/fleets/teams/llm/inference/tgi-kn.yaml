# apiVersion: serving.knative.dev/v1
# kind: Service
# metadata:
#   name: tgi-gemma-deployment
#   namespace: inference
# spec:
#   template:
#     metadata:
#       labels:
#         app: gemma-server
#         ai.gke.io/model: gemma-2b-1.1-it
#         ai.gke.io/inference-server: text-generation-inference
#         examples.ai.gke.io/source: user-guide
#       annotations:
#         autoscaling.knative.dev/min-scale: "1"
#         autoscaling.knative.dev/initial-scale: "1"
#     spec:
#       nodeSelector:
#         cloud.google.com/gke-accelerator: nvidia-tesla-a100
#       containers:
#         - name: inference-server
#           image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-hf-tgi-serve:20240328_0936_RC01
#           resources:
#             requests:
#               cpu: "2"
#               memory: "7Gi"
#               ephemeral-storage: "20Gi"
#               nvidia.com/gpu: 1
#             limits:
#               cpu: "2"
#               memory: "7Gi"
#               ephemeral-storage: "20Gi"
#               nvidia.com/gpu: 1
#           args:
#           - --model-id=$(MODEL_ID)
#           - --num-shard=1
#           env:
#           - name: MODEL_ID
#             value: google/gemma-1.1-2b-it
#           # - name: PORT
#           #   value: "8000"
#           - name: HUGGING_FACE_HUB_TOKEN
#             valueFrom:
#               secretKeyRef:
#                 name: hf-secret
#                 key: hf_api_token
#           volumeMounts:
#           - mountPath: /dev/shm
#             name: dshm
#           ports:
#           - containerPort: 8080
#       volumes:
#       - name: dshm
#         emptyDir:
#           medium: Memory
